# Argo with Connect Gateway Commands

PROJECT ID : iac-prd-v1

gcloud compute start-iap-tunnel INSTANCE_NAME INSTANCE_PORT \
    --local-host-port=localhost:LOCAL_PORT \
    --zone=ZONE

gcloud compute start-iap-tunnel bastion-v1 22 \
    --local-host-port=localhost:9090 \
    --zone=europe-west2-c


# Tunnel to the bastion host through proxy

gcloud compute ssh bastion-vm-1 \
    --tunnel-through-iap \
    --project=project-a-347105 \
    --zone=us-central1-a  \
    --ssh-flag="-4 -L8888:localhost:8888 -N -q -f"


gcloud compute ssh bastion-v1 \
    --tunnel-through-iap
    
gcloud compute ssh bastion-v1 --zone=europe-west2-c --tunnel-through-iap -- -L 9090:172.16.52.34:443 -N -f

kubectl config set-cluster gke_iac-prd-v1_europe-west2_autopilot-cluster-1 --server=https://kubernetes:9090
gcloud compute ssh bastion-v1 --zone=europe-west2-c --project=iac-prd-v1 --tunnel-through-iap -- -o ServerAliveInternal=5 -o ServerAliveCountMax=1 -o StrictHostKeyChecking=no -L 9090:172.16.52.34:443 -N -f

To kill the process on Port 9090
lsof -nP -iTCP:9090 -sTCP:LISTEN -t | xargs -r kill -9

# On the Central Cluster Project  OR Fleet Host Project (This wil have the master Argo CD)
gcloud iam service-accounts create argocd-fleet-admin --project $PROJECT_ID

gcloud projects add-iam-policy-binding $PROJECT_ID --member "serviceAccount:argocd-fleet-admin@${PROJECT_ID}.iam.gserviceaccount.com" --role roles/gkehub.gatewayEditor

gcloud iam service-accounts add-iam-policy-binding --role roles/iam.workloadIdentityUser --member "serviceAccount:${PROJECT_ID}.svc.id.goog[argocd/argocd-server]" argocd-fleet-admin@$PROJECT_ID.iam.gserviceaccount.com

gcloud iam service-accounts add-iam-policy-binding --role roles/iam.workloadIdentityUser --member "serviceAccount:${PROJECT_ID}.svc.id.goog[argocd/argocd-application-controller]" argocd-fleet-admin@$PROJECT_ID.iam.gserviceaccount.com

gcloud services enable --project=$PROJECT_ID gkeconnect.googleapis.com gkehub.googleapis.com cloudresourcemanager.googleapis.com iam.googleapis.com ​​connectgateway.googleapis.com




###########################################
## On The Application Clusters 
##########################################

# PROJECT_ID = Central Cluster Project (Fleet Project)
# PROJECT_ID_1 = Application Cluster Porject

# To Set the applications cluster project
gcloud config set project $PROJECT_ID_1
gcloud services enable --project=$PROJECT_ID_1 gkeconnect.googleapis.com gkehub.googleapis.com cloudresourcemanager.googleapis.com iam.googleapis.com ​​connectgateway.googleapis.com
gcloud container clusters list --uri --project $PROJECT_ID_1

# Register th Applciation Cluster with Fleet Project
gcloud container fleet memberships register $CLUSTER_NAME --gke-uri {URI from step 2} --enable-workload-identity --project $PROJECT_ID

# To Check if the cluster has been regisgered to the fleet
gcloud container fleet memberships list --project $PROJECT_ID


#### To allow GKE HUb Service Account to register a cluster from other account into the Fleet ###


gcloud projects add-iam-policy-binding "${FLEET_HOST_PROJECT_ID}" \
  --member "serviceAccount:service-${FLEET_HOST_PROJECT_NUMBER}@gcp-sa-gkehub.iam.gserviceaccount.com" \
  --role roles/gkehub.serviceAgent

# To ge the Project Number of Fleet Project

gcloud projects describe "${PROJECT_ID}"

# Udating as per our enviornment
gcloud projects add-iam-policy-binding "${PROJECT_ID}" \
  --member "serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-gkehub.iam.gserviceaccount.com" \
  --role roles/gkehub.serviceAgent


gcloud projects add-iam-policy-binding "${PROJECT_ID_1}" \
  --member "serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-gkehub.iam.gserviceaccount.com" \
  --role roles/gkehub.serviceAgent

## Update the annotiations for Argo CD server to use the new fleet sevice accounts
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    iam.gke.io/gcp-service-account: argocd-fleet-admin@$PROJECT_ID.iam.gserviceaccount.com
  name: argocd-application-controller
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    iam.gke.io/gcp-service-account: argocd-fleet-admin@$PROJECT_ID.iam.gserviceaccount.com
  name: argocd-server


# Create a Secret

apiVersion: v1
kind: Secret
metadata:
  name: cluster-app
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: cluster
type: Opaque
stringData:
  name: cluster-app
  server: https://connectgateway.googleapis.com/v1beta1/projects/iac-prd-v1/locations/global/gkeMemberships/autopilot-cluster-v2
  config: |
    {
      "execProviderConfig": {
        "command": "argocd-k8s-auth",
        "args": ["gcp"],
        "apiVersion": "client.authentication.k8s.io/v1beta1"
      },
      "tlsClientConfig": {
        "insecure": false,
        "caData": ""
      }
    }

# Create a test application on ArgoCD


kubectl port-forward svc/argocd-server -n argocd 8080:443
kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
HUdVLhZW3FjWOx-l

gcloud container fleet memberships list --project=iac-prd-v1

Unable to create application: error while validating and normalizing app: error validating the repo: error getting k8s server version: parse "http://https://connectgateway.googleapis.com/v1beta1/projects/\tiac-prd-v1/locations/global/gkeMemberships/autopilot-cluster-v2": net/url: invalid control character in URL
Unable to create application: error while validating and normalizing app: error validating the repo: error getting k8s server version: Get "https://connectgateway.googleapis.com/v1beta1/projects/iac-prd-v1/locations/global/gkeMemberships/autopilot-cluster-v2/version?timeout=32s": getting credentials: exec: executable argocd-k8s-auth failed with exit code 20